<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Jekyll Serial Programmer | 선형 회귀 분석</title>
  
  <link rel="stylesheet" href="/assets/css/post.css" />
  <link rel="stylesheet" href="/assets/css/syntax.css" />
  
  <link rel="stylesheet" href="/assets/css/common.css" />
  <script src="/assets/js/categories.js"></script>
  
  <script defer src="/assets/js/lbox.js"></script>
  
</head>

  <body>
    <main>
      <header>
  <a class="site-title" href="/">Jekyll Serial Programmer</a>
</header>

      <section class="article">
        <div class="article-header">
          <h2 class="article-title">선형 회귀 분석</h2>
          <small class="date">13 Jun 2021</small>
          <div class="categories">
            
            <a href="#!" class="category">ML</a>
            
          </div>
        </div>
        <div class="content"><h2 id="선형-회귀-분석">선형 회귀 분석</h2>

<p>데이터 분석 교과 과정을 수강하면서 회귀분석 중 계수를 추정하는 점수에 대해 이해가 부족하다는 사실을 깨닫고 늦게나마 관련 공부를 했습니다.</p>

<h3 id="선형-회귀-분석이란">선형 회귀 분석이란</h3>

<p>회귀 분석이란 관측한 특징 X를 기반으로 해당 특징이 결과에 미치는 영향의 정도에 대해서는 가중치 W로 가정하여 문제를 해결하는 방법입니다.</p>

<p>여기서 선형이라는 단어는 공부한 바로는 특징과 예측값의 그래프 자체가 선형 관계라는 뜻이 아닌 가중치에 해당하는 W가 선형적인 특징이 있다는 것으로 이해하고 있습니다.</p>

<h3 id="선형-회귀-분석-1">선형 회귀 분석</h3>

<p>실제로 테스트 하면서 알아보겠습니다.</p>

<p>단순한 선형 회귀를 보여주는 그래프는 다음과 같습니다. 문제에 대해서 특징 x의 값에 대해 예측값 y_hat과 실제 정답인 y의 오차가 가장 적은 선을 그리는 것이 목표입니다.</p>

<p>이 말만 들으면 뭔가 이해가 안되니 공식을 사용해봅시다. 먼저 선형적인 관계를 나타내는 1차 함수를 정의해봅시다.</p>

\[\hat{y} = wx + b\]

<p>그리고 동일한 데이터가 주어졌을 때의 각각의 데이터를 공식에 넣어보겠습니다.</p>

\[0 = 0w + b\]

\[2 = 1w + b\]

\[4 = 2w + b\]

\[5 = 3w + b\]

<p>두 번째 공식까지는 w값과 b값을 쉽게 추정할 수 있습니다. <strong>w가 2이며 동시에 b가 0이라면 두 공식은 완벽히 정답</strong>에 일치하게 됩니다. 하지만 마지막 공식에서 이 추정은 무너지게 되죠, <strong>세 번째 공식에서 동일한 w와 b값으로 연산을 하게 되면 예측한 h_hat은 6이 됩니다.</strong></p>

<p>그러면 오차를 구해봅시다. 오차를 구하는 방법은 사실 다양한 방법이 있습니다만, 주로 사용하는 오차는 MSE(Mean Square Error)입니다. 공식으로 적으면 다음과 같습니다.</p>

\[MSE = {1\over{n}} \sum_{i=1}^{n}({y_i - \hat{y_i}})^2\]

<p>정답과 예측한 예측값과의 차를 제곱한 값을 모두 더한 뒤 데이터의 수로 나누어 주면 에러를 알 수 있습니다. 통계학적으로 데이터의 수로 나누어주는 것은 즉 수 많은 모수에서 관측한 표본의 수를 다루는 것이기에 n-1로 나누기도 합니다만, 어차피 오차를 얻기 위함이며 변동하는 값이기에 신경쓰지 않아도 될 듯 합니다.</p>

<p>그러면 오차를 줄여보도록 하겠습니다. 우리는 이 학습 과정에서 알아야 할 개념이 있습니다. 조금 더 깊은 내용에 대해서는 이후에 또 다른 포스트에서 다루도록 하겠습니다만 Gradient Descent를 통해서 우리는 w값이 이동함에 따라서 오차(Loss)가 얼마나 증감하는지 기울기를 통해 예측할 수 있습니다.</p>

<p>일단 짧게 <strong>“w값의 변화로 인한 오차(Loss)가 얼마나 영향을 받을지에 대해 그래프의 기울기를 통해 어느정도 예측 할 수 있다.”</strong> 로 줄이겠습니다. 말 그래도 어느정도만 예측 가능합니다. <strong>참고로 해당 글에서는 앞으로 Loss를 사용하겠습니다.</strong> 엄밀히 말하면 오차와 Loss는 차이가 있습니다. <em>오차는 관측값과 정답에 대한 차이값에 대해 통합적으로 나타내는 말이지만 Loss는 정해진 Loss Function에 따라 달라지는 값이라고 배웠습니다. 아닐 수 있습니다</em></p>

<p>그러면 우리는 Loss가 적어지는 방향으로 학습을 진행합니다.</p>

<p>다시 그래프로 돌아가보겠습니다. 두 번째 공식까지만으로 그려진 그래프를 해당 데이터의 위치에 맞게 선을 그러보면 다음과 같은 그림이 됩니다.</p>

<p><img src="/public/img/회귀1.png" width="300" height="400" /></p>

<p>그러면 오차가 발생하죠, 세 번째 데이터와 그래프 위의 회귀 선은 1의 오차가 있고 MSE 값으로 보면 3분의 1의 Loss가 발생합니다.</p>

<p>Loss를 줄이기 위해 w값을 수정해보겠습니다. 다음은 w(기울기)를 수정한 그래프 입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="nb">reduce</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">]</span>

<span class="n">line_y</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">6.0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.9</span><span class="p">,</span><span class="mf">3.8</span><span class="p">,</span><span class="mf">5.7</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.8</span><span class="p">,</span><span class="mf">3.6</span><span class="p">,</span><span class="mf">5.4</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.7</span><span class="p">,</span><span class="mf">3.4</span><span class="p">,</span><span class="mf">5.1</span><span class="p">]]</span>

<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="n">set_size_inches</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">line_y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">error</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">err</span><span class="p">,</span> <span class="n">cur</span> <span class="p">:</span> <span class="n">err</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">cur</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">line_y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">,</span> <span class="mi">0</span> <span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"MSE = "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="o">/</span><span class="mi">4</span><span class="p">)[:</span><span class="mi">5</span><span class="p">])</span>

</code></pre></div></div>

<p><img src="/public/img/회귀2.png" width="1000" height="400" /></p>

<p>그래도 이상한 점이 있습니다. 바로 이해하셨겠지만 모든 선이 0을 지나가게 되면서 그릴 수 있는 그래프의 한계가 명확한 점이 보이실겁니다. 이때! 우리의 b(편향)이 일을 시작합니다.</p>

<p>두 그림의 차이를 보시면 왜 편향이 중요한 지 알 수 있습니다.</p>

<p>우리는 b값도 학습에 따라 바뀌는 값 임을 알 수 있습니다. 그러면 공식을 바꾸어서 나타내는게 더 이해가 빠를 것 같습니다.</p>

\[\hat{y} = \beta_0 + \beta_1x_1\]

<p>똑같은 1차 방정식이지만 우리가 찾아야 할 값이 2개임을 나타냈습니다. MSE의 공식도 다음과 같이 수정 할 수 있습니다.</p>

\[MSE = {1\over{n}} \sum_{i=1}^{n}({y_i - \beta_0 - \beta_1x_1})^2\]

<p>그리고 특징 $x$의 갯수가 $k$개 일 경우에는 다음과 같이 공식을 일반화 시킬 수 있습니다.</p>

\[MSE = {1\over{n}} \sum_{i=1}^{n}({y_i - \beta_0 - \beta_1x_1} - \beta_2x_2 ... \beta_kx_k)^2\]

<p><strong>우리는 이 값이 최소화되는 $\beta$를 찾으면 가장 데이터를 잘 나타내는 선형 방정식을 구할 수 있습니다.</strong></p>

<p>마지막으로 최적의 $\beta_0$과 $\beta_1$을 찾게 되면 다음과 같은 그래프를 그릴 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">line_fitter</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">line_fitter</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">line_fitter</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'coef = '</span><span class="p">,</span><span class="n">line_fitter</span><span class="p">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'intercept = '</span><span class="p">,</span><span class="n">line_fitter</span><span class="p">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y_predict = '</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">)</span>

<span class="n">error</span> <span class="o">=</span> <span class="nb">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">err</span><span class="p">,</span> <span class="n">cur</span> <span class="p">:</span> <span class="n">err</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">cur</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y_predicted</span><span class="p">)</span> <span class="p">,</span> <span class="mi">0</span> <span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"MSE = "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="o">/</span><span class="mi">4</span><span class="p">)[:</span><span class="mi">5</span><span class="p">])</span>

<span class="c1"># y = 1.7x + 0.200
</span></code></pre></div></div>

<p><img src="/public/img/회귀3.png" width="400" height="400" /></p>

<p>위의 그림은 편향도 학습하여 Loss를 최소화 시킨 그림입니다. 확연히 Loss가 줄어든 것을 확인할 수 있습니다. 이 그림에서는 눈에 띄지 않더라도 데이터가 많아지면 더욱 더 눈에 띄는 차이를 알 수 있습니다.</p>

<h3 id="선형-회귀-분석을-진행-할-때-주의할-점">선형 회귀 분석을 진행 할 때 주의할 점</h3>

<p>선형 회귀는 해당 특징이 가진 가중치를 얻는 방법이기에 사용할 때 명심해야 하는 점이 있습니다.</p>

<h4 id="1-특징독립변수는-정말-독립적인가">1. 특징(독립변수)는 정말 독립적인가?</h4>

<p>특징은 그 자체로써의 특징을 가지고 있어야 합니다. 예를들어 다음과 같은 특징이 있을 경우 선형회귀는 좋은 특징을 학습하지 못할 수 있습니다.</p>

<p><strong>점수과 등급</strong> - 등급은 점수에 따라 높게 책정됩니다. 즉 당연히 점수가 높은 사람은 등급이 높게 책정되며, 이 경우에는 올바른 가중치를 갖지 못할 가능성이 있습니다. 즉 특징으로써 가중치를 신뢰 할 수 없습니다. 이 경우에는 두 특징 중 하나만을 사용하도록 혹은 두 값을 더하는 방식을 이용하여 사용하도록 해야합니다.</p>

<p>독립변수간의 상관관계가 있는지를 판별하기 위해서 우리는 <strong>다중공선성</strong>을 이용하여 판단할 수 있습니다.</p>

<p>이런 특징을 파악하려면 특징들간의 선형관계를 확인해보면 어느정도 알 수 있다는 것으로 알고 있습니다.</p>

<h4 id="2-문제가-선형적인가">2. 문제가 선형적인가?</h4>

<p>문제 자체가 선형적이지 않는 문제들이 있습니다. 이 경우에는 고차원의 복잡한 모델로 학습을 하더라도 문제에 맞는 모델이 만들어지지 않을 경우가 있습니다. 물론 매우 고차원의 모델을 사용하면 학습이 가능할 수 있지만 과적합의 위험이 높아지게 됩니다.</p>

<h4 id="3-문제를-해결하기-위한-특징은-충분히-수집되었는가">3. 문제를 해결하기 위한 특징은 충분히 수집되었는가?</h4>

<p>선형회귀의 공식에는 특징과 특징이 가지고 있는 가중치의 곱들의 합이 포함되어 있으며 동시에 찾지 못한 특징에 대해 판단하기 위해 상수를 더하는 과정이 포함되어 있습니다. 즉, <strong>현재 관측한 특징으로 설명되지 않는 다른 특징이 있을 것이라는 가정</strong>이 포함되어 있습니다.</p>

<p><strong>입실론 혹은 편차</strong> 라고 부르는 이 상수가 우리에게 말해주는 바는 관측된 특징과 그에 대한 가중치만으로 설명되는 <strong>분산(학습으로 줄일 수 있는 오차)</strong> 와는 다르게 학습을 하더라도 줄일 수 없는 오차가 있다는 점을 시사합니다.</p>

<p>만약에 특징의 갯수에 비해서 데이터 수가 너무나 적거나 혹은 특징이 문제를 해결하기에 좋은 특징이 아닐 경우에는 이 상수가 커짐으로써 우리는 문제에 대해 좋은 모델을 만들 수 없습니다.</p>

<h4 id="실제로-확인해보자">실제로 확인해보자</h4>

<p>테스트에 사용한 데이터는 집 값 데이터를 이용해보겠습니다. 데이터는 <a href="https://github.com/BaekKyunShin/Data-Analyst-Nanodegree">BaekKyunShin 님의 깃허브</a>에서 가져왔습니다.</p>

<p>BeakKyunShin님께서 분석한 내용을 참고해서 간단히 분석해보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">patsy</span> <span class="kn">import</span> <span class="n">dmatrices</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span><span class="p">;</span>
<span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./house_prices.csv'</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
<span class="n">df</span><span class="p">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/public/img/회귀4.png" width="500" height="500" /></p>

<p>데이터 상태를 보니 이웃에 대한 점수와 집의 스타일에 대한 데이터는 카테고리 형태로 이루어져 있으니 일단은 분석의 범주에서 제외하고 추후에 완성된 모델에서 사용하도록 하겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'intercept'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'price'</span><span class="p">],</span> <span class="n">df</span><span class="p">[[</span><span class="s">'intercept'</span><span class="p">,</span> <span class="s">'bedrooms'</span><span class="p">,</span> <span class="s">'bathrooms'</span><span class="p">,</span> <span class="s">'area'</span><span class="p">]])</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">lm</span><span class="p">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">results</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/public/img/회귀5.png" width="500" height="500" /></p>

<p>위의 분석표는 <strong>방의 개수 / 화장실의 개수 / 면적을 이용하여 집 값과의 관계를 분석한 결과입니다.</strong></p>

<p>먼저 <strong>결정계수 R-squared</strong>를 보면 0.678입니다. 이 결정계수는 현재 독립변수는 종속변수인 집 값에 대해 0.678정도 표현할 수 있다는 추정치는 알려주고 있습니다.</p>

<p>주로 선형회귀 분석을 할 때 분산과 편향을 기반으로 분석을 진행하며 결정계수는 선형회귀의 적합도를 나타내는 지표로 사용되고 있습니다. 예측값에 대한 분산을 기반으로 구하는 것으로 이해하고 있습니다만, 추가적으로 공부하겠습니다.</p>

<p>결정계수는 0부터 1까지의 수를 가지며 0에 가까울수록 해당 선형회귀는 적합하지 않다는 뜻이며 1에 가까울수록 적합한 선형회귀입니다. 단 극단으로 1에 가까우면 데이터의 수가 너무 적은게 아닌지 검증을 해 볼 필요성이 있다고 조심스럽게 의견을 제시해봅니다.</p>

<p><strong>또 하나 중요한 것은 intercept 부분입니다.</strong> 두 번째 표를 보면 실제로 분석을 한 요소가 아닌 요소로 슬쩍 끼어있는 것을 볼 수 있습니다.</p>

<p><strong>이것은 우리가 아직 찾지 못한 특징입니다. 중요하니까 한번 더 적겠습니다 우리가 아직 찾지 못한 특징입니다.</strong></p>

<p>거의 모든 모델이 완벽할 수 없는 이유인 intercept는 모델이 가진 한계점입니다.</p>

<p>집 값을 정하는 요소가 과연 방의 갯수 혹은 화장실의 갯수, 땅의 면적이라고 단정지을 수 있을까요? 그렇다면 저는 지방 지역과 10배에 가까운 집 값을 가진 서울에 대해서 전혀 해석 할 수 없을 것입니다.</p>

<p>혹시 우리가 해당 데이터를 조금 더 깊게 분석하기 위해 <strong>-편향을 줄이기 위해-</strong> 더 많은 데이터를 수집해보겠습니다. 예를 들면 <strong>대형마트와의 거리 / 대형병원과의 거리 / 여러 기피시설과의 거리 데이터를 모두 수집했다고 하겠습니다.</strong></p>

<p>수집한 데이터가 집 값에 영향을 끼치는 유의미한 관측치라면 데이터를 더 잘 설명할 수 있게 되고, 모델은 더욱 유연한 그래프를 그리게 됩니다. 동시에 설명할 수 없는 값인 편향은 감소하게 됩니다.</p>

<ol>
  <li>독립적이지 않는 특징으로 분석하는 결과 ( 가중치의 차이 값 )</li>
</ol>

<p>위의 문제에서 이상한 점을 하나 발견할 수 있습니다. 바로 두 번째 표에 등장한 bedrooms의 가중치입니다. 그대로 해석한다면 <strong>침실의 개수가 많아질수록 집 값은 감소한다.</strong> 이상합니다.</p>

<p>우리는 두 가지 방법을 이용해서 다중공선성을 확인할 예정입니다. 바로 <strong>VIF와 상관계수입니다.</strong> 사실 VIF만 이용해도 괜찮습니다.(더 정확할거에요)</p>

<p>독립변수들을 scatter 그래프로 표현해보겠습니다.</p>

<p><img src="/public/img/회귀6.png" width="500" height="500" /></p>

<ol>
  <li>선형적이지 않는 데이터를 분석한 결과</li>
</ol>

<p>추가예정</p>

<ol>
  <li>문제를 해결하기 위한 특징이 부족한 결과 ( 그래프 자체가 무의미 )</li>
</ol>

<p>추가예정</p>

<h4 id="참고">참고</h4>

<p><a href="https://heung-bae-lee.github.io/2020/01/04/machine_learning_01/">https://heung-bae-lee.github.io/2020/01/04/machine_learning_01/</a></p>

<p><a href="https://woochan-autobiography.tistory.com/135">https://woochan-autobiography.tistory.com/135</a></p>

<p><a href="https://todayisbetterthanyesterday.tistory.com/8">https://todayisbetterthanyesterday.tistory.com/8</a></p>

<p><a href="https://agronomy4future.com/2020/10/27/회귀분석의-결정계수-r-squared-를-가장-쉽게-설명해-보자/">https://agronomy4future.com/2020/10/27/회귀분석의-결정계수-r-squared-를-가장-쉽게-설명해-보자/</a></p>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>

</div>
      </section>
      <footer>
  <p>&copy; 1947 - 2022 | Jekyll Serial Programmer</p>
</footer>

    </main>
    <section id="category-modal-bg"></section>
<section id="category-modal">
  <h1 id="category-modal-title"></h1>
  <section id="category-modal-content"></section>
</section>

  </body>
</html>
