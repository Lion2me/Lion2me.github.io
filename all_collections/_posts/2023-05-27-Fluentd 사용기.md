---

layout: post
title: Fluentd를 사용 한 이벤트 데이터 수집
date: 2023-06-10 21:05:23 +0900
category: DE
use_math: true

---

# Fluentd를 사용 한 이벤트 데이터 수집

이 글은 현업에서 Fluentd를 사용하여 이벤트 데이터를 수집 및 DW에 적재하는 과정에서 공부 한 내용을 정리하는 글 입니다.

## 이벤트 로그 데이터를 수집 한다는 것

Fluentd를 활용하기 전 데이터를 수집하는 것에 대해 이야기를 하면 좋을 것 같습니다.

데이터의 소스는 다양한 곳에서 추출 할 수 있습니다. 웹 상의 데이터를 크롤링하여 얻는 방법, REST API로 제공하고 있는 데이터를 HTTP Request로 얻는 방법 등 다양합니다.

하지만 위의 두 방법은 **이미 존재하는 데이터를 수집**하는 방법에 속한다면 저는 **존재하지 않았지만 현재 발생하는 정보**를 얻는 방법에 관해서 이야기 할 예정입니다.

**"현재 발생하는 정보"는 다시말해 "연속적으로 발생하는 정보"**라고 할 수 있습니다. 데이터를 배치로 처리를 하든 스트림으로 처리를 하든 데이터의 발생 자체가 연속적인 데이터를 처리하게 됩니다.

예를 들면 이러한 로그들과 같은 데이터 입니다.

```
C 유저가 접속 한 로그
B 유저가 접속 한 로그
B 유저가 A` 제품을 구매 한 로그
A 유저가 접속 한 로그
B 유저가 B` 제품을 구매 한 로그
A 유저가 A` 제품을 구매 한 로그
C 유저가 장바구니에 A` 물건을 등록 한 로그
```

위 데이터는 시간의 순서대로 (네트워크 등의 문제로 그렇지 않을 수 있지만) 발생하는 이벤트의 로그입니다. 저는 이러한 데이터를 수집하여 배치로 DW에 적재하는 파이프라인을 구축했습니다.

## 데이터 수집기란

데이터를 수집하는 방법은 여러가지 방안을 사용 할 수 있습니다. GET Request를 받은 Nginx와 같은 웹 서버에서 로그 형태로 수집하여 읽는 방법도 있고, Request 자체를 REST API 서버를 구축하여 데이터를 받을수도 있습니다. 하지만 이렇게 개발하게되면 어떠한 서비스를 확장 할 때 개별적으로 확장해야 합니다.

수집 한 데이터를 ElasticSearch로 적재하려면 그에 맞는 데이터 형식으로 변환해야하고, HDFS 서버로 전송하려면 해당 서버로 전송하여 CopyFromLocal을 수행해야 합니다.

확장을 포함하여 빠르게 개발하기 위해 공통의 로직으로 획일화 하여 만든 프로그램이 Fluentd와 Logstash와 같은 데이터 수집기 입니다.

## Fluentd를 사용 한 이유

위에서 저는 2가지의 데이터 수집기에 대해 이야기 했습니다. 그러면 왜 Fluentd를 사용했는지를 설명해야 문맥이 맞을 것이라고 생각합니다.

### Logstash

Logstash는 JVM 위에서 동작하는 JRuby로 된 데이터 수집기입니다. ElasticSearch를 만든 Elastic 에서 만들었으며, 그렇기 때문에 ElasticSearch와 매우 호환성이 좋은 수집기입니다.

여기서 호환성이 좋다는 말을 덧붙였는데, 실제로 Fluentd를 사용하면서 Elastic과 관련한 플러그인을 봤는데 데이터의 형식을 맞추기 위한 설정들이 꽤나 많은 걸 볼 수 있었습니다. 그에 비해서 Logstash는 그리 어렵지 않게 ElasticSearch로 데이터를 색인 할 수 있죠.

그런데 JVM위에서 동작하는 점이 조심스럽습니다. 기기나 운영체제에 따른 호환성을 따지면 JVM은 정답에 가깝지만, 하나의 데이터가 하나의 객체로 힙 영역에 쌓이면서 처리 이후 GC의 대상이 된다는 점은 처리량 관점에서 영향이 있을 가능성이 있습니다.

그러면 문제를 제대로 확인해보아야 합니다.

#### 1. 우리는 ElasticSearch를 사용 할 예정인가?

초기에는 사용 할 수 있을 것이라고 생각했지만, 팀원간의 의사소통 이후 사용 가능성이 낮다고 판단했습니다.

#### 2. 우리가 데이터 수집기를 활용 할 서버의 환경이 유동적인가?

이 또한 유동적이지 않습니다. IDC에서 온프레미스 서버로 활용하는 환경은 리눅스 환경으로 고정적이였으며, 특정 플랫폼이나 운영체제를 사용할 예정은 없었습니다.

설령 AWS의 인스턴스를 사용하더라도 리눅스 환경에서 동작 시킬 예정이이였습니다.

그러면 굳이 메모리 사용량과 처리량 측면에서 불리 할 가능성이 있는 Logstash를 사용 할 이유는 없다고 생각했습니다.

### Fluentd

Fluentd는 CRuby를 활용하는 데이터 수집기로 C언어 계열이기 때문에 JVM과 같은 부하를 굳이 감수 할 필요가 없습니다. 그리고 오픈소스로 다양한 플러그인을 활용하여 많은 플랫폼과의 상호작용을 기대 할 수 있습니다.

운영체제에 따라 Fluentd의 활용이 어려운 부분은 있지만, 리눅스 환경에서는 대부분 Fluentd의 활용에는 어렵지 않습니다. 결과적으로 일간 약 1억 5천만건의 데이터를 처리하는 수집기를 개발하기위한 적합한 도구라고 판단했습니다.

## Fluentd에 대한 내용

Fluentd를 사용하게 되었으니 더 자세한 내용을 포함하고자 합니다.

<https://docs.fluentd.org/>

공식 문서를 확인하면 알 수 있듯, Fluentd는 문서가 정말 깔끔하게 정리되어 있으며 많은 플러그인이 있는 것을 확인 할 수 있습니다.

동작과정을 표현하면 다음과 같습니다.

<img src="/assets/img/fluentd_process.png" width="800" height="400">

위의 프로세스는 Fluentd의 프로세스 안에서 동작하는 방식입니다. 이제 하나씩 분해하면서 그 과정을 따라가보도록 하겠습니다.

### 1. Input

Input은 Source 데이터를 가져오는 과정입니다. 여기에는 HTTP Request(POST)의 요청을 받은 서버로의 동작으로 만들 수 있으며, 파일을 tail -f하여 디스크에 쓰여지는 로그를 읽어 낼 수 있습니다.

뿐만 아니라, Kafka나 RabbitMQ와 같은 메시지를 Consume하는 것도 하나의 입력으로 활용 될 수 있습니다.

그리고 중요 한 점은 이렇게 받은 데이터는 Fluentd에서 공통으로 활용하는 형태로 바뀌게 됩니다. 그 형태는 바로 **Timestamp와 Tag, Record입니다.**

#### Timestamp

서버가 해당 데이터를 입력받은 시점의 서버 시간입니다. 대부분 문제는 없지만 여기서 중요한 것은 **서버 시간**이라는 점 입니다.

저는 다국가의 데이터를 다루었기 때문에 **서버 시간보다 이벤트의 발생 시간**이 중요한 부분이였습니다. 그래서 실제로 이 Timestamp 대신 로그 내 시간을 활용했습니다. 하지만 이러한 Timestamp도 변경 할 수 있는 플러그인이 있기 때문에 그걸 활용하면 됩니다.

#### Tag

Tag는 데이터가 속해있는 그룹으로 생각하면 됩니다. 이후 로직에서 데이터를 처리하는 과정에서 **우리가 어떤 데이터를 처리 할 것인가?**의 답을 내는 것이 이 Tag입니다.

예를 들면 유저의 데이터만을 별도로 처리하도록 한다면, Tag의 값을 **"user_{group}"**과 같이 설정한 뒤에 **우리는 user\_로 시작하는 데이터를 처리 할 것이다**는 속성을 설정하면 됩니다.

#### Record

이 값은 실제로 입력받은 데이터를 담고있는 속성입니다. 형식으로는 Python의 Dict, Java의 Map이라고 생각하시면 이해하기 편할 것 같습니다. 실제로 Ruby언어로 해당 필드의 값을 바꾸면 그와 비슷하게 동작합니다.

이러한 Record 내의 값을 변경하거나 추가하는 등의 플러그인은 Fluentd에서 공식 플러그인으로 지원하고 있습니다.

### 2. Parser (Optional)

Parser는 Input에서 Record를 지정하는 부분으로 생각하면 편할 것 같습니다. 위에서 Record는 실제로 입력받은 데이터를 저장하고 있는 부분이라고 했지만, 그 데이터가 어떤 것인지 알기 어려 울 수 있습니다.

POST Request의 경우에는 Json 형태로 입력받으면 Record로 변환하기에 편합니다. 이유는 Record 자체가 Json의 형식과 비슷하기 때문이죠. 하지만 Nginx의 access.log는 어떨까요? 이러한 문제가 Parser가 Optional인 이유입니다.

Nginx는 로그의 형식을 자유롭게 지정 할 수 있습니다. 문제는 그 로그의 형식에 따라서 데이터를 파싱해야하는데 어떻게 파싱 할 수 있는가입니다.

여기서 사용하는 것은 **정규표현식**입니다.

정규 표현식을 사용하여 각 데이터를 부분으로 나누어서 컬럼값을 포함하게 하면 Fluentd는 자동으로 Record 형식으로 변환해줍니다. 뿐만 아니라 Timestamp의 시간도 Nginx의 로그 시간으로 설정하도록 해줍니다.

### 3. Filter (Optional)

Filter는 말그대로

